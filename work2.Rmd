---
title: "Attrition Classification"
author: "Syed Ehraaz Atif"
date: "2024-05-15"
output:
  html_document:
    df_print: paged
---

```{r}
library(e1071)
```

This dataset was obtained from: "https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers". The objective of the dataset is to figure out why customers are leaving (attriting), and what could be done to decrease the number of customers leaving.\
\
Here are the variables in the dataset, with necessary explanations:\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) Attrition_Flag -> whether a customer has churned or not\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) Customer_Age\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) Gender\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4) Depdendent_Count -> number of dependents of customer\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5) Education_Level\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6) Marital_Status\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7) Income_Category\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8) Card_Category -> what type of card the customer has with the bank\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9) Months_on_book\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10) Total_Relationship_Count -> total number of relationships the customer has; what defines a "relationship" is unfortunately not mentioned in the metadata\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11) Months_Inactive_12_mon\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12) Contacts_Count_12_mon -> number of times customer has contacted the bank in the past 12 months\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;13) Credit_Limit\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14) Total_Revolving_Bal\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15) Avg_Open_To_Buy\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16) Total_Amt_Chng_Q4_Q1\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;17) Total_Trans_Amt\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;18) Total_Trans_Ct\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19) Total_Ct_Chng_Q4_Q1\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20) Avg_Utilization_Ratio\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;21) Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1 -> previous person's work, to be removed\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;23) Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2 -> previous person's work, to be removed\
\
**DATA CLEANING**

```{r}
##load data
df <- read.csv("BankChurners.csv")
head(df)
```

```{r}
##get rid of unnecessary columns
df <- df[,-c(1,22,23)]
colnames(df)
```

```{r}
##inspect variables
cols <- c(2,4,9:20)
for (i in cols) {
  boxplot(df[,i], xlab=colnames(df)[i])
}
```

```{r}
##inspect variables
unique(df$Attrition_Flag)
unique(df$Gender)
unique(df$Education_Level)
unique(df$Marital_Status)
unique(df$Income_Category)
unique(df$Card_Category)
```

Some of the categorical variables seem to have "Unknown" values, which are probably a stand-in for NA or missing values.

```{r}
##get dataset where each row has at least one "Unknown"
edu_unknown <- which(df$Education_Level == "Unknown")
marital_unknown <- which(df$Marital_Status == "Unknown")
income_unknown <- which(df$Income_Category == "Unknown")
rows_unknown <- unique(union(unique(union(edu_unknown, marital_unknown)), income_unknown))

length(rows_unknown)
```

There are 3046 observations with "Unknown" in at least one column. The total size of the dataset is ~10k observations. If all 3046 observations are removed, there would still be ~70% of the dataset leftover (about 7k observations), which should be more than enough to do any analysis or modeling.

```{r}
##take out unknowns
df2 <- df[-as.integer(rows_unknown),]
dim(df2)
```


```{r}
##convert appropriate columns to factor datatype
df2$Attrition_Flag <- factor(df2$Attrition_Flag)
df2$Gender <- factor(df2$Gender)
df2$Marital_Status <- factor(df2$Marital_Status)

df2$Education_Level <- factor(df2$Education_Level,
                              levels=c(
                                "Uneducated",
                                "High School",
                                "College",
                                "Graduate",
                                "Post-Graduate",
                                "Doctorate"
                              ), ordered=TRUE)

df2$Income_Category <- factor(df2$Income_Category,
                              levels=c(
                                "Less than $40K",
                                "$40K - $60K",
                                "$60K - $80K",
                                "$80K - $120K",
                                "$120K +"
                              ), ordered=TRUE)

df2$Card_Category <- factor(df2$Card_Category,
                            levels=c(
                              "Blue",
                              "Silver",
                              "Platinum",
                              "Gold"
                            ), ordered=TRUE)
```

```{r}
##inspect variables
barplot(height=table(df2$Attrition_Flag))
barplot(height=table(df2$Gender))
barplot(height=table(df2$Education_Level))
barplot(height=table(df2$Marital_Status))
barplot(height=table(df2$Income_Category))
barplot(height=table(df2$Card_Category))
```

Observations:\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) Many more existing customers than attrited\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) Very few divorced in comparison to married or single\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) Very, very few non-blue card members\
\
To balance out the dataset, an equivalent number of attrited and existing customers must be sampled, as an imbalance could be problematic for any models. There should remain ~2k observations, which is a lot less than the initial 10k, but is still a sizeable amount. Additionally, the vast majority of customers have blue cards, so that variable gives negligeble information; it might be simpler to remove it entirely. Lastly, although there are imbalances in the Marital_Status variable, they are not significant enough to consider re-balancing.

```{r}
##balance out the dataset
set.seed(0)

##sample existing customers
attrited_customer_sample <- rownames(df2[df2$Attrition_Flag=="Attrited Customer",])
existing_customer_sample <- sample(rownames(df2[df2$Attrition_Flag=="Existing Customer",]),
                                   length(attrited_customer_sample))
sample_rows <- union(attrited_customer_sample, existing_customer_sample)
df3 <- df2[sample_rows,]

##exclude the card category
df3 <- df3[,-8]
```

**EDA & MODELING**\
\
Now that the data is clean, we can begin to investigate relationships between the variables.

```{r}
##look for relationships
##keeping it commented out - interesting plots are displayed below

#for (i in 1:19) {
#  for (j in 1:19) {
#    if (i != j) {
#      plot(x=df3[,i], y=df3[,j], xlab=colnames(df3)[i], ylab=colnames(df3)[j])
#    }
#  }
#}
```

```{r}
##found the following relationships
plot(Gender ~ Attrition_Flag, data=df3)
plot(Income_Category ~ Attrition_Flag, data=df3)
plot(Total_Relationship_Count ~ Attrition_Flag, data=df3)
plot(Months_Inactive_12_mon ~ Attrition_Flag, data=df3)
plot(Contacts_Count_12_mon ~ Attrition_Flag, data=df3)
plot(Total_Revolving_Bal ~ Attrition_Flag, data=df3)
plot(Total_Trans_Amt ~ Attrition_Flag, data=df3)
plot(Total_Trans_Ct ~ Attrition_Flag, data=df3)
plot(Total_Ct_Chng_Q4_Q1 ~ Attrition_Flag, data=df3)
plot(Avg_Utilization_Ratio ~ Attrition_Flag, data=df3)
plot(Months_on_book ~ Customer_Age, data=df3)
plot(Income_Category ~ Gender, data=df3)
plot(Total_Relationship_Count ~ Gender, data=df3)
plot(Credit_Limit ~ Gender, data=df3)
plot(Avg_Open_To_Buy ~ Gender, data=df3)
plot(Avg_Utilization_Ratio ~ Gender, data=df3)
plot(Gender ~ Education_Level, data=df3)
plot(Total_Relationship_Count ~ Education_Level, data=df3)
plot(Dependent_count ~ Marital_Status, data=df3)
plot(Total_Relationship_Count ~ Marital_Status, data=df3)
plot(Credit_Limit ~ Income_Category, data=df3)
plot(Avg_Open_To_Buy ~ Income_Category, data=df3)
plot(Avg_Utilization_Ratio ~ Income_Category, data=df3)
plot(Avg_Open_To_Buy ~ Credit_Limit, data=df3)
plot(Avg_Utilization_Ratio ~ Credit_Limit, data=df3)
plot(Avg_Utilization_Ratio ~ Avg_Open_To_Buy, data=df3)
plot(Total_Trans_Ct ~ Total_Trans_Amt, data=df3)
```

Interesting notes (things that were interesting and were not obvious or self-explanatory):\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) even though females occupy higher income categories as well, they have a lower median credit limit than males\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) average utilization ratio DECREASES as credit limit INCREASES -> very strange\
\
Although there are other relationships as well, the main objective of the dataset is to figure out how to decrease customer churn. For this reason, only relationships where Attrition_Flag is the response variable will be modeled. Of the relationships depicted above, Total_Trans_Ct will be excluded because it is collinear to Total_Trans_Amt.

```{r}
##make a model investigating relationships to Attrition_Flag
model_1 <- glm(Attrition_Flag ~ Gender +
                 Income_Category +
                 Total_Relationship_Count +
                 Months_Inactive_12_mon +
                 Contacts_Count_12_mon +
                 Total_Revolving_Bal +
                 Total_Trans_Amt +
                 Total_Ct_Chng_Q4_Q1 +
                 Avg_Utilization_Ratio, data=df3, family="binomial")
```

```{r}
##check if model assumptions hold
plot(model_1)
```

The residuals are not randomly scattered about 0, and the variance is non-constant. Transformations to the predictors may help with this.

```{r}
model_2 <- glm(Attrition_Flag ~ Gender +
                 Income_Category +
                 Total_Relationship_Count +
                 Months_Inactive_12_mon +
                 Contacts_Count_12_mon +
                 Total_Revolving_Bal +
                 Total_Trans_Amt +
                 I(Total_Ct_Chng_Q4_Q1^2) +
                 Avg_Utilization_Ratio, data=df3, family="binomial")

plot(model_2)
```

Raising Total_Ct_Chng_Q4_Q1 to a degree of 2 made things a bit better. Points 2511 and 758 seem very problematic for the model.

```{r}
df3[rownames(df3) %in% c(2511, 758),]
```

It seems like the Months_Inactive_12_mon and Avg_Utilization_Ratio columns do not reconcile for these 2 observations. The Months_Inactive_12_mon column indicates that the customers were quite active throughout the year, yet their Avg_Utilization_Ratio's are both 0. One of the columns (probably Avg_Utilization_Ratio) seems to have incorrect data, which is negatively impacting the model. These points should be removed.

```{r}
##try removing the point and see new model's performance
df3_clean <- df3[!rownames(df3) %in% c(2511, 758),]

model_3 <- glm(Attrition_Flag ~ Gender +
                 Income_Category +
                 Total_Relationship_Count +
                 Months_Inactive_12_mon +
                 Contacts_Count_12_mon +
                 Total_Revolving_Bal +
                 Total_Trans_Amt +
                 I(Total_Ct_Chng_Q4_Q1^2) +
                 Avg_Utilization_Ratio, data=df3_clean, family="binomial")

plot(model_3)
```

Unfortunately the model did not improve, and the basic assumptions still do not hold. After removing points 2511 and 758, there are now new outliers. They could also be removed, but there is no guarantee that the model assumptions will hold true. On top of that, continuously removing outliers to try and improve the model is bad practice - the data may be true, but by removing it information about the relationships between the variables is lost.\
\
One alternative way to use a model to determine which variables are most important for predicting customer churn (in this scenario) would be to use an SVM. SVMs use the given features and add transformations to try and form a decision boundary. While using an SVM, if the number of predictors used are minimzed, then the model can tell us something about which predictors are the most important for customer churn. The only disadvantage would be that there would be no interpretability like there is in logistic regression (ex. increasing X by 1 unit leads to a change of A in likelihood).

```{r}
##scale variables - prepare for SVM
df4 <- apply(df3[,-c(1,3,5,6,7)], FUN=scale, MARGIN=2)
df4 <- cbind(df4, df3[,c(1,3,5,6,7)])

##split train and test sets
set.seed(0)
train_size <- as.integer(nrow(df4) * 0.7)
train_index <- sample(1:nrow(df4), train_size)
train <- df4[train_index,]
test <- df4[-train_index,]
```

```{r}
##try SVM using the initial variables
model_4 <- svm(Attrition_Flag ~ Gender +
                 Income_Category +
                 Total_Relationship_Count +
                 Months_Inactive_12_mon +
                 Contacts_Count_12_mon +
                 Total_Revolving_Bal +
                 Total_Trans_Amt +
                 Total_Ct_Chng_Q4_Q1 +
                 Avg_Utilization_Ratio, data=train, kernel="radial", scale=FALSE)

summary(model_4)
```


```{r}
##test the predictive power
preds <- predict(model_4, newdata=test)
table(test$Attrition_Flag, preds)
```

The model is already quite accurate, with a misclassification rate of 18%.

```{r}
##try SVM with all variables
model_5 <- svm(Attrition_Flag ~ . - Customer_Age - Credit_Limit, data=train, kernel="radial", scale=FALSE)
summary(model_5)
```

```{r}
##make confusion matrix for SVM
preds <- predict(model_5, newdata=test)
table(test$Attrition_Flag, preds)
```

Adding some predictors made the model more accurate.\
\
Now, a minimal predictive model will be built - maximizing the predictive power while minimizing the number of predictors used. Not only will this help identify which variables are most important for predicting customer churn, but it makes real life applications easier. If an accurate prediction can be made with as little predictors as possible, then the number of use cases in which the model can be used increases, as it becomes more generalized.

```{r}
##perform backward step-wise feature selection using misclassification rate
pred_cols <- c(2:6, 8:14, 16:19)
removed <- c()
formula <- "Attrition_Flag ~ ."
errors <- c()
while (length(pred_cols) > 0) {
  worst_predictor <- NULL
  worst_error <- 1
  for (col in pred_cols) {
    col_name <- colnames(train)[col]
    test_formula <- paste(formula, "-", col_name)
    model_svm <- svm(formula(test_formula), data=train, kernel="radial", scale=FALSE)
    preds <- predict(model_svm, newdata=test)
    error <- mean(preds != test$Attrition_Flag)
    
    if (error < worst_error) {
      worst_predictor <- col
      worst_error <- error
    }
  }
  
  remove <- pred_cols[which(pred_cols==worst_predictor)]
  pred_cols <- pred_cols[-which(pred_cols==worst_predictor)]
  removed <- c(removed, remove)
  formula <- paste(formula, "-", colnames(train)[remove])
  errors <- c(errors, worst_error)
}
```

```{r}
##plot the errors to look for trends
plot(errors, type="o")
```

It seems like the misclassification rate is quite low even after removing 10 predictors, after which the error starts to increase exponentially.

```{r}
first_10 <- removed[1:10]
colnames(train)[first_10]
```

By removing these 10 predictors, there should be an accurate model while minimizing the misclassification rate. Which means that the following predictors:

```{r}
colnames(train)[-first_10]
```

are the most important, excluding Customer_Age and Credit_Limit due to multicollinearity, and Attrition_Flag for obvious reasons.\

```{r}
##construct new model with "important" predictors
model_6 <- svm(Attrition_Flag ~ Months_Inactive_12_mon + 
                 Total_Revolving_Bal + 
                 Total_Amt_Chng_Q4_Q1 + 
                 Total_Trans_Amt + 
                 Total_Trans_Ct + 
                 Total_Ct_Chng_Q4_Q1, data=train, kernel="radial", scale=FALSE)

summary(model_6)
```

```{r}
##construct confusion matrix
preds <- predict(model_6, newdata=test)
table(test$Attrition_Flag, preds)
```

This model has the following metrics:\
Accuracy: 91%\
Misclassification Rate: 9%\
Precision: 93%\
Recall: 89%\
\
By all metrics, the model is very good.\
\
**CONCLUSION**\
\
The purpose of this dataset was to determine why customers were churning. According to this objective, the following variables were found to have relationships with customer churn (according to plots):\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) Gender\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) Income_Category\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) Total_Relationship_Count\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4) Months_Inactive_12_mon\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5) Contacts_Count_12_mon\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6) Total_Revolving_Bal\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7) Total_Trans_Amt\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8) Total_Trans_Ct\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9) Total_Ct_Chng_Q4_Q1\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10) Avg_Utilization_Ratio\
Unfortunately, no model could be built to gain numerical inferences about the above relationships.\
\
Another way of figuring out which variables were most important in predicting customer churn would be to use an SVM (which automatically adds transformations), then minimzing the number of predictors used in the model. To accomplish this, a method similar to "backward step-wise feature selection" was used, with misclassification rate as the comparison metric. The following predictors were found to be the most useful:\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) Months_Inactive_12_mon\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) Total_Revolving_Bal\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) Total_Amt_Chng_Q4_Q1\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4) Total_Trans_Amt\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5) Total_Trans_Ct\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6) Total_Ct_Chng_Q4_Q1\
Notably, these predictors are a near-subset of the predictors that were initially found to have the strongest relationships with customer churn. The only exception is Total_Amt_Chng_Q4_Q1, but that is not too surprising as it is probably related to Total_Ct_Chng_Q4_Q1. The SVM reinforces that most of the relationships that were found graphically were significant and correct.